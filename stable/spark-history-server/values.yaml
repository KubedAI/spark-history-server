# Default values for spark-history-server.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: ghcr.io/kubedai/spark-history-server
  pullPolicy: Always
  # Overrides the image tag whose default is the chart appVersion.
  tag: aws-latest # or "azure-latest" or your specific version

  # To pull images from the registries like ghcr.io, you need to authenticate and provide the imagePullSecrets as described here:
  # https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  # If you have already created a secret for pulling images from ghcr.io, you can specify it in property imagePullSecrets.
  # If you don't have a secret yet, you can create one using the property pullCredentials.

  # See the pullSecrets field within the container definitions of a Pod: https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#containers
  pullSecrets: [ ]

  # If you want the Helm chart to take care of the secret creation, generate a personal access token as described here:
  # https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-to-the-container-registry
  pullCredentials:
    enabled: false
    secretName: ghcr-io-pull-secret
    registry: ghcr.io
    username: <GITHUB USERNAME>
    password: <GITHUB PERSONAL ACCESS TOKEN>
    email: <GITHUB EMAIL>

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  name: spark-history-server-sa
  annotations: { }

logStore:
  # Enter the type of log store to use. Supported values are "s3", "abfs", and "local".
  # Remember
  type: "s3"  # or "abfs" or "local" depending on which storage you're targeting

  s3:
    # Ensure IRSA roles has permissions to read the files for the given S3 bucket
    bucket: "example-bucket"  # This is just a placeholder for publishing
    eventLogsPath: "spark-events/"  # This is just a placeholder for publishing
    irsaRoleArn: "arn:aws:iam::123456789012:role/example-role"  # This is just a placeholder for publishing

  abfs:
    container: <ABFS CONTAINER NAME>
    storageAccount: <STORAGE ACCOUNT NAME>
    clientId: <CLIENT ID>
    clientSecret: <CLIENT SECRET>
    tenantId: <TENANT ID>
    eventLogsPath: <PREFIX FOR SPARK EVENT LOGS>
    
  local:
    directory: "/home"

# Extra configuration for spark-defaults.conf
#sparkConf: |-
#  spark.ui.proxyBase=/history

log4jConfig: |-
  rootLogger.level = info
  rootLogger.appenderRef.stdout.ref = console
  appender.console.type = Console
  appender.console.name = console
  appender.console.target = SYSTEM_ERR
  appender.console.layout.type = PatternLayout
  appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex

  logger.spark.name = org.apache.spark
  logger.spark.level = INFO
  logger.hadoop.name = org.apache.hadoop
  logger.hadoop.level = INFO
  logger.s3a.name = org.apache.hadoop.fs.s3a
  logger.s3a.level = DEBUG
  logger.history.name = org.apache.spark.deploy.history.FsHistoryProvider
  logger.history.level = DEBUG

historyServer:
  fs:
    update:
      interval: 10s
    cleaner:
      enabled: false
      interval: 1d
      maxAge: 7d
      maxNum: 2147483647 #Int.MaxValue
    endEventReparseChunkSize: 1m
    inProgressOptimization:
      enabled: true
    driverlog:
      cleaner:
        enabled: false
        interval: 1d
        maxAge: 7d
    eventLog:
      rolling:
        maxFilesToRetain: 2147483647 # Int.MaxValue
  retainedApplications: 50
  ui:
    maxApplications: 2147483647 # Int.MaxValue


podAnnotations: { }

# Extra custom labels for pods
podLabels: { }

podSecurityContext:
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000

service:
  externalPort: 80
  internalPort: 18080
  type: ClusterIP

ingress:
  enabled: false
  annotations: { }
  ingressClassName: <INGRESS CLASS NAME>
  hosts:
    - host: <YOUR HOST NAME>
      paths:
        - path: "^/$"
#         pathType: ImplementationSpecific

resources:
  limits:
    cpu: 200m
    memory: 2G
  requests:
    cpu: 100m
    memory: 1G

livenessProbe:
  httpGet:
    path: /
    port: 18080
    scheme: HTTP
  timeoutSeconds: 5
  periodSeconds: 30
  successThreshold: 1
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /
    port: 18080
    scheme: HTTP
  timeoutSeconds: 5
  periodSeconds: 30
  successThreshold: 1
  failureThreshold: 3

nodeSelector: { }

tolerations: [ ]

affinity: { }
